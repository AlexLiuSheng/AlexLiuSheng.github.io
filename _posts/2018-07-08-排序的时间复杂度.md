 ## 排序时间复杂度

### 时间复杂度概念
同一个问题可以用不同的算法来解决，一个算法的开销以及时间记做：T(n),n表示算法的基本操作模块执行的**次数**，算法的时间复杂度记做T(n)=O(f(n)),随着n增大，执行时间的增长率和f(n)增长率成正比，所以f(n)越小，算法时间就越少，效率也就越高，时间复杂度用大O表述，这种表述叫大O介表示法
### 时间复杂度计算规则
- 用常数1替代运行时间计算的所有加法常数
- 修改之后次数函数中，只保留高阶项
- 如果高阶项存在且不是1.去除与这个项相乘的常数
也就是说当

### 如何计算时间复杂度

```
int n = 100000; //执行了1次
for(int i = 0; i < n; i++){  //执行了n+1次
  for(int j = 0; j < n; j++) //执行了n*(n+1)次
  {
      printf("i = %d, j = %d", i, j); //执行了n*n次
  }
}

for(int i = 0; i < n; i++){  //执行了n+1次
  printf("i = %d", i); //执行了n次
}

printf("Done"); //执行了1次
```
- 直接将操作执行次数相加 1+n+1+n*(n+1)+n^2+n+1+n+1
- 根据计算规则1 去除加法常数=3n+2n^2+1
- 根据计算规则2 只保留高阶项=2n^2
- 根据计算规则3 去除相乘常数=n^2
- 所以这个算法的时间复杂度为O(n^2)

### 总结
一个算法所耗费的时间=算法中每条语句的执行时间之和。算法转换为程序后，每条语句执行一次所需的时间取决于机器的指令性能、速度以及编译所产生的代码质量等难以确定的因素。如果要计算一个算法执行所耗费的时间，从理论上是不能算出来的，必须上机运行测试才能知道。
若要独立于机器的软、硬件系统来分析算法的时间耗费，则设每条语句执行一次所需的时间均是单位时间，一个算法的时间耗费就是该算法中所有语句的频度之和（算法中的语句执行次数称为语句的频度或时间频度）。




